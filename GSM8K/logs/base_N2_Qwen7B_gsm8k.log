Using device: cuda
正在加载语言模型...
Loading checkpoint shards:   0%|                                                                                                                                                                                                    | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████████████████████████████                                                                                                                                             | 1/4 [00:01<00:05,  1.95s/it]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████                                                                                              | 2/4 [00:02<00:02,  1.25s/it]Loading checkpoint shards:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                               | 3/4 [00:03<00:01,  1.15s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.00s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.13s/it]
语言模型加载完成!
正在加载数据集...
数据集加载完成，共5个样本
开始执行基于整个回答置信度的评估...
Processing: 0it [00:00, ?it/s]Processing: 1it [00:19, 19.30s/it]Processing: 2it [00:26, 12.47s/it]Processing: 3it [00:34, 10.12s/it]Processing: 3it [00:41, 13.71s/it]
Error generating candidates for sample 0: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 31.47 GiB of which 17.31 MiB is free. Including non-PyTorch memory, this process has 15.36 GiB memory in use. Process 155857 has 16.08 GiB memory in use. Of the allocated memory 14.79 GiB is allocated by PyTorch, and 278.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error generating candidates for sample 1: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.47 GiB of which 19.31 MiB is free. Including non-PyTorch memory, this process has 14.87 GiB memory in use. Process 155857 has 16.57 GiB memory in use. Of the allocated memory 14.46 GiB is allocated by PyTorch, and 119.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error generating candidates for sample 2: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.47 GiB of which 19.31 MiB is free. Including non-PyTorch memory, this process has 14.87 GiB memory in use. Process 155857 has 16.57 GiB memory in use. Of the allocated memory 14.45 GiB is allocated by PyTorch, and 126.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/base.py", line 107, in <module>
    main()
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/base.py", line 87, in main
    Response_Certainty_Selection(
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/methods/response_certainty.py", line 142, in Response_Certainty_Selection
    candidates = generate_with_transformers(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/utils/common.py", line 156, in generate_with_transformers
    outputs = model.generate(input_ids, **generate_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py", line 2629, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py", line 3613, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py", line 959, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 450, in forward
    outputs: BaseModelOutputWithPast = self.model(
                                       ^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py", line 1083, in wrapper
    outputs = func(self, *args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 379, in forward
    hidden_states = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 231, in forward
    hidden_states, _ = self.self_attn(
                       ^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 156, in forward
    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 79, in apply_rotary_pos_emb
    k_embed = (k * cos) + (rotate_half(k) * sin)
                           ^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 53, in rotate_half
    return torch.cat((-x2, x1), dim=-1)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Using device: cuda
正在加载语言模型...
Loading checkpoint shards:   0%|                                                                                                                                                                                                    | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████████████████████████████                                                                                                                                             | 1/4 [00:00<00:01,  2.23it/s]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████                                                                                              | 2/4 [00:00<00:00,  2.72it/s]Loading checkpoint shards:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                               | 3/4 [00:01<00:00,  3.01it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.38it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  3.10it/s]
语言模型加载完成!
正在加载数据集...
数据集加载完成，共2个样本
开始执行基于整个回答置信度的评估...
Processing: 0it [00:00, ?it/s]Processing: 0it [00:10, ?it/s, accuracy=1.0000]Processing: 1it [00:10, 10.16s/it, accuracy=1.0000]Processing: 1it [00:15, 10.16s/it, accuracy=1.0000]Processing: 2it [00:15,  7.51s/it, accuracy=1.0000]Processing: 2it [00:15,  7.91s/it, accuracy=1.0000]

--- 样本 1 调试信息 ---
问题: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
真实答案: 18
模型答案(提取): 18
清理后文本(保存的 answer): To determine how much Janet makes every day at the farmers' market, we need to follow these steps: 1. **Calculate the total number of eggs Janet receives per day:** Janet’s ducks lay 16 eggs per day. 2. **Determine the number of eggs Janet eats per day:** Janet eats 3 eggs for breakfast every morning. 3. **Determine the number of eggs Janet uses for baking muffins per day:** Janet bakes muffins for her friends every day with 4 eggs. 4. **Calculate the total number of eggs Janet uses per day:** \[ \text{Total eggs used per day} = \text{Eggs eaten per day} + \text{Eggs used for baking muffins per day} = 3 + 4 = 7 \] 5. **Calculate the number of eggs Janet sells per day:** Janet sells the remainder of the eggs she receives after eating and using some for baking muffins. \[ \text{Eggs sold per day} = \text{Total eggs received per day} - \text{Total eggs used per day} = 16 - 7 = 9 \] 6. **Determine the amount of money Janet makes from selling the eggs:** Janet sells each fresh duck egg for $2. \[ \text{Money made per day} = \text{Eggs sold per day} \times \text{Price per egg} = 9 \times 2 = 18 \] Therefore, Janet makes \(\boxed{18}\) dollars every day at the farmers' market.
最高置信度: 19.2259
是否正确: True
--- 结束调试信息 ---


--- 样本 2 调试信息 ---
问题: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?
真实答案: 3
模型答案(提取): 3
清理后文本(保存的 answer): To determine the total number of bolts of fiber needed for the robe, we need to follow these steps: 1. Identify the amount of blue fiber required. 2. Calculate the amount of white fiber required. 3. Add the amounts of blue and white fiber together. Step 1: Identify the amount of blue fiber required. The problem states that the robe takes 2 bolts of blue fiber. Step 2: Calculate the amount of white fiber required. The problem also states that the robe takes half as much white fiber as blue fiber. Since the blue fiber is 2 bolts, the white fiber is: \[ \frac{2}{2} = 1 \text{ bolt} \] Step 3: Add the amounts of blue and white fiber together. The total number of bolts of fiber is: \[ 2 + 1 = 3 \] Therefore, the total number of bolts of fiber needed for the robe is \(\boxed{3}\).
最高置信度: 22.0770
是否正确: True
--- 结束调试信息 ---

########################################################################################
Accuracy of Model@2: 1.0000.
Total samples: 2, Correct: 2
Elapsed time: 15.82 secs.
########################################################################################
Results saved to ./TTT_data/Self_Certainty_Trajectorylevel_best_of_2_Qwen7B_GSM8K.json
########################################################################################
实验配置参数:
########################################################################################
                   method ===> response-certainty
    n_repetitive_sampling ===> 2
              temperature ===> 0.7
                    top_p ===> 1.0
               model_path ===> Qwen/Qwen2.5-Math-7B-Instruct
             save_to_json ===> True
        dataset_repo_name ===> openai/gsm8k
               max_tokens ===> 512
              subset_size ===> 2
########################################################################################
