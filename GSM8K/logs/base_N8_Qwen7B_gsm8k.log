Using device: cuda
正在加载语言模型...
Loading checkpoint shards:   0%|                                                                                                                                                                                                    | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████████████████████████████                                                                                                                                             | 1/4 [00:00<00:01,  2.47it/s]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████                                                                                              | 2/4 [00:00<00:01,  1.96it/s]Loading checkpoint shards:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                               | 3/4 [00:02<00:00,  1.25it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.07s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:03<00:00,  1.10it/s]
语言模型加载完成!
正在加载数据集...
数据集加载完成，共5个样本
开始执行基于整个回答置信度的评估...
Processing: 0it [00:00, ?it/s]Processing: 1it [00:10, 10.11s/it]Processing: 2it [00:20, 10.14s/it]Processing: 3it [00:30, 10.29s/it]Processing: 3it [00:38, 12.73s/it]
Error generating candidates for sample 0: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.47 GiB of which 19.31 MiB is free. Process 155378 has 15.26 GiB memory in use. Including non-PyTorch memory, this process has 16.18 GiB memory in use. Of the allocated memory 15.33 GiB is allocated by PyTorch, and 560.39 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error generating candidates for sample 1: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.47 GiB of which 19.31 MiB is free. Process 155378 has 15.24 GiB memory in use. Including non-PyTorch memory, this process has 16.20 GiB memory in use. Of the allocated memory 15.34 GiB is allocated by PyTorch, and 576.77 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Error generating candidates for sample 2: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 31.47 GiB of which 11.31 MiB is free. Process 155378 has 15.25 GiB memory in use. Including non-PyTorch memory, this process has 16.20 GiB memory in use. Of the allocated memory 15.37 GiB is allocated by PyTorch, and 542.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/base.py", line 107, in <module>
    main()
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/base.py", line 87, in main
    Response_Certainty_Selection(
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/methods/response_certainty.py", line 142, in Response_Certainty_Selection
    candidates = generate_with_transformers(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/autodl-tmp/Multi-step-Test-time-Training/GSM8K/utils/common.py", line 156, in generate_with_transformers
    outputs = model.generate(input_ids, **generate_kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py", line 2629, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/generation/utils.py", line 3613, in _sample
    outputs = model_forward(**model_inputs, return_dict=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py", line 959, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 450, in forward
    outputs: BaseModelOutputWithPast = self.model(
                                       ^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/utils/generic.py", line 1083, in wrapper
    outputs = func(self, *args, **kwargs)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 379, in forward
    hidden_states = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 231, in forward
    hidden_states, _ = self.self_attn(
                       ^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/models/qwen2/modeling_qwen2.py", line 167, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/integrations/flash_attention.py", line 64, in flash_attention_forward
    attn_output = _flash_attention_forward(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/root/miniconda3/lib/python3.12/site-packages/transformers/modeling_flash_attention_utils.py", line 612, in _flash_attention_forward
    flash_kwargs = process_flash_kwargs_fn(
                   ^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Using device: cuda
正在加载语言模型...
Loading checkpoint shards:   0%|                                                                                                                                                                                                    | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|███████████████████████████████████████████████                                                                                                                                             | 1/4 [00:00<00:01,  2.82it/s]Loading checkpoint shards:  50%|██████████████████████████████████████████████████████████████████████████████████████████████                                                                                              | 2/4 [00:00<00:00,  3.13it/s]Loading checkpoint shards:  75%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                               | 3/4 [00:00<00:00,  3.39it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.50it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.71it/s]
语言模型加载完成!
正在加载数据集...
数据集加载完成，共2个样本
开始执行基于整个回答置信度的评估...
Processing: 0it [00:00, ?it/s]Processing: 0it [00:13, ?it/s, accuracy=1.0000]Processing: 1it [00:13, 13.41s/it, accuracy=1.0000]Processing: 1it [00:23, 13.41s/it, accuracy=1.0000]Processing: 2it [00:23, 11.30s/it, accuracy=1.0000]Processing: 2it [00:23, 11.62s/it, accuracy=1.0000]

--- 样本 1 调试信息 ---
问题: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?
真实答案: 18
模型答案(提取): 18
清理后文本(保存的 answer): To determine how much Janet makes every day at the farmers' market, we need to follow these steps: 1. Calculate the total number of eggs Janet has each day. 2. Determine how many eggs Janet eats and uses each day. 3. Calculate the number of eggs remaining each day. 4. Calculate the total revenue from selling the remaining eggs. Let's go through each step in detail: 1. **Calculate the total number of eggs Janet has each day:** Janet's ducks lay 16 eggs per day. 2. **Determine how many eggs Janet eats and uses each day:** Janet eats 3 eggs for breakfast every morning and bakes muffins for her friends every day with 4 eggs. So, the total number of eggs she uses each day is \(3 + 4 = 7\). 3. **Calculate the number of eggs remaining each day:** The number of eggs remaining each day is the total number of eggs laid minus the number of eggs eaten and used. So, the number of eggs remaining each day is \(16 - 7 = 9\). 4. **Calculate the total revenue from selling the remaining eggs:** Janet sells each remaining egg for $2. So, the total revenue from selling the remaining eggs is \(9 \times 2 = 18\). Therefore, Janet makes \(\boxed{18}\) dollars every day at the farmers' market.
最高置信度: 19.6513
是否正确: True
--- 结束调试信息 ---


--- 样本 2 调试信息 ---
问题: A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?
真实答案: 3
模型答案(提取): 3
清理后文本(保存的 answer): To determine the total number of bolts of fiber needed for the robe, we need to follow these steps: 1. Identify the amount of blue fiber required. 2. Calculate the amount of white fiber required. 3. Add the amounts of blue and white fiber together. Let's go through each step in detail: 1. **Identify the amount of blue fiber required:** The problem states that the robe takes 2 bolts of blue fiber. 2. **Calculate the amount of white fiber required:** The problem also states that the robe takes half as much white fiber as blue fiber. Since the blue fiber is 2 bolts, we calculate the white fiber as follows: \[ \text{White fiber} = \frac{1}{2} \times 2 = 1 \text{ bolt} \] 3. **Add the amounts of blue and white fiber together:** Now, we add the blue fiber and the white fiber to find the total number of bolts: \[ \text{Total bolts} = 2 + 1 = 3 \] Therefore, the total number of bolts of fiber needed for the robe is \(\boxed{3}\).
最高置信度: 21.4297
是否正确: True
--- 结束调试信息 ---

########################################################################################
Accuracy of Model@8: 1.0000.
Total samples: 2, Correct: 2
Elapsed time: 23.23 secs.
########################################################################################
Results saved to ./TTT_data/Self_Certainty_Trajectorylevel_best_of_8_Qwen7B_GSM8K.json
########################################################################################
实验配置参数:
########################################################################################
                   method ===> response-certainty
    n_repetitive_sampling ===> 8
              temperature ===> 0.7
                    top_p ===> 1.0
               model_path ===> Qwen/Qwen2.5-Math-7B-Instruct
             save_to_json ===> True
        dataset_repo_name ===> openai/gsm8k
               max_tokens ===> 512
              subset_size ===> 2
########################################################################################
