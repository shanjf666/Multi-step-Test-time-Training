"""
Pass@1 evaluation main function for MATH-500 dataset (vLLM version, silent mode, batch processing)
"""

import os
import re
import sys
import json
import time
import argparse
import contextlib
from tqdm import tqdm

from datasets import load_dataset
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

import torch

# ç¦ç”¨ä¸å¿…è¦çš„ä¼˜åŒ–
os.environ["TRITON_ALLOW_MMA"] = "0"
os.environ["NVIDIA_TF32_OVERRIDE"] = "0"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

from core.config import Config
from utils.common import (
    extract_model_answer,
    is_correct_answer,
    clean_latex_format
)


# ==========================
# é™éŸ³ä¸Šä¸‹æ–‡ç®¡ç†å™¨
# ==========================
@contextlib.contextmanager
def suppress_vllm_output():
    """ä¸´æ—¶å±è”½ vLLM çš„æ§åˆ¶å°è¾“å‡ºï¼ˆåŒ…æ‹¬ Rich è¿›åº¦æ¡ï¼‰"""
    with open(os.devnull, "w") as devnull:
        old_stdout, old_stderr = sys.stdout, sys.stderr
        sys.stdout, sys.stderr = devnull, devnull
        try:
            yield
        finally:
            sys.stdout, sys.stderr = old_stdout, old_stderr


def pass_at_1_evaluation_vllm(dataset, config, llm, tokenizer, save_results=False, batch_size=8):
    """
    ä½¿ç”¨ vLLM æ‰¹é‡ç”Ÿæˆå®ç° Pass@1 è¯„ä¼°å‡½æ•°ï¼ˆé™éŸ³ + æ‰¹å¤„ç†ç‰ˆæœ¬ï¼‰
    """
    table = []
    n_true_ans = 0
    n_samples = 0
    start = time.time()
    index = 0

    # vLLM é‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=config.temperature,
        top_p=1.0,
        max_tokens=config.max_tokens
    )

    progress_bar = tqdm(range(0, len(dataset), batch_size), desc="Processing", leave=False)

    for start_idx in progress_bar:
        batch = dataset[start_idx : start_idx + batch_size]

        # âœ… å°† Batch è½¬æ¢ä¸ºæ ·æœ¬å­—å…¸åˆ—è¡¨
        batch_data = [
            {k: v[i] for k, v in batch.items()}
            for i in range(len(batch['problem']))
        ]

        prompts = []
        for data in batch_data:
            question = data['problem']
            prompt = tokenizer.apply_chat_template(
                [{"role": "user", "content": f"Q: {question}\nLet's think step by step and the final answer within \\boxed{{}}\nA:"}],
                tokenize=False,
                add_generation_prompt=True
            )
            prompts.append(prompt)

        # ğŸš« æ‰¹é‡ç”Ÿæˆï¼Œé™éŸ³
        with suppress_vllm_output():
            outputs = llm.generate(prompts, sampling_params)

        for i, data in enumerate(batch_data):
            true_answer = clean_latex_format(data['answer'])
            response_text = outputs[i].outputs[0].text

            # æ¸…ç†ç”Ÿæˆæ–‡æœ¬
            cleaned_text = re.sub(r'<\|eot_id\|>', '', response_text)
            cleaned_text = re.sub(r'<\|start_header_id\|>.*?<\|end_header_id\|>', '', cleaned_text, flags=re.DOTALL)
            cleaned_text = re.sub(r'<\|.*?\|>', '', cleaned_text)
            cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()

            # æå–æ¨¡å‹ç­”æ¡ˆ
            model_answer = extract_model_answer(cleaned_text)
            model_answer = clean_latex_format(model_answer)
            is_correct = is_correct_answer(model_answer, true_answer)
            if is_correct:
                n_true_ans += 1
            n_samples += 1

            # å­˜å‚¨ç»“æœ
            table.append({
                "ID": index + 1,
                "question": data['problem'],
                "response": cleaned_text,
                "model_answer": model_answer,
                "true_answer": true_answer,
                "is_correct": is_correct
            })
            index += 1

            # å‰å‡ ä¸ªæ ·æœ¬æ‰“å°è°ƒè¯•ä¿¡æ¯
            if index <= 3:
                print(f"\n--- æ ·æœ¬ {index} è°ƒè¯•ä¿¡æ¯ ---")
                print(f"é—®é¢˜: {data['problem']}")
                print(f"çœŸå€¼ç­”æ¡ˆ: {true_answer}")
                print(f"æ¨¡å‹ç­”æ¡ˆ: {model_answer}")
                print(f"æ¨¡å‹ç”Ÿæˆæ–‡æœ¬: {cleaned_text}")
                print(f"æ˜¯å¦æ­£ç¡®: {is_correct}")
                print("--- ç»“æŸè°ƒè¯•ä¿¡æ¯ ---\n")

        # æ›´æ–°è¿›åº¦æ¡
        acc_display = f"{(n_true_ans / n_samples):.4f}" if n_samples > 0 else "0.0000"
        progress_bar.set_postfix(accuracy=acc_display)

    end = time.time()
    accuracy = n_true_ans / n_samples if n_samples > 0 else 0.0
    print("########################################################################################")
    print(f"Pass@1 Accuracy: {accuracy:.4f}")
    print(f"Total samples: {n_samples}, Correct: {n_true_ans}")
    print(f"Elapsed time: {end - start:.2f} secs.")
    print("########################################################################################")

    if save_results:
        os.makedirs("./TTT_data", exist_ok=True)
        output_file = "./TTT_data/pass_at_1_results_vllm.json"
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump({
                "results": table,
                "accuracy": accuracy,
                "total_samples": n_samples,
                "correct_samples": n_true_ans
            }, f, indent=4, ensure_ascii=False)
        print(f"Results saved to {output_file}")

    return accuracy



def main():
    config = Config()
    parser = argparse.ArgumentParser(description="Pass@1 evaluation using vLLM (silent, batch mode)")
    parser.add_argument("--temperature", default=1, type=float)
    parser.add_argument("--model_path", default="Qwen/Qwen2.5-Math-1.5B-Instruct")
    parser.add_argument("--save_to_json", action="store_true")
    parser.add_argument("--dataset_repo_name", default="HuggingFaceH4/MATH-500")
    parser.add_argument("--max_tokens", default=1024, type=int)
    parser.add_argument("--subset_size", default=None, type=int)
    parser.add_argument("--batch_size", default=8, type=int)
    args = parser.parse_args()

    config.temperature = args.temperature
    config.max_tokens = args.max_tokens

    print("æ­£åœ¨åŠ è½½è¯­è¨€æ¨¡å‹ (vLLM)...")
    # ğŸš« ç”¨ suppress_vllm_output é™éŸ³åŠ è½½æ¨¡å‹
    with suppress_vllm_output():
        llm = LLM(model=args.model_path, dtype="half", gpu_memory_utilization=0.9)
    tokenizer = AutoTokenizer.from_pretrained(args.model_path)
    print("æ¨¡å‹åŠ è½½å®Œæˆï¼")

    print("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    dataset = load_dataset(args.dataset_repo_name, "default", split="test")
    if args.subset_size:
        dataset = dataset.select(range(min(args.subset_size, len(dataset))))
    print(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")

    print("å¼€å§‹æ‰§è¡ŒPass@1è¯„ä¼°...")
    pass_at_1_evaluation_vllm(dataset, config, llm, tokenizer, save_results=args.save_to_json, batch_size=args.batch_size)

    print("########################################################################################")
    print("å®éªŒé…ç½®å‚æ•°:")
    for arg, value in vars(args).items():
        print(f"{arg:>25} ===> {value}")
    print("########################################################################################")


if __name__ == "__main__":
    main()
